{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "converter = DocumentConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176fa7354106412e858067105a62ea2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "from docling_core.transforms.chunker import HierarchicalChunker\n",
    "\n",
    "sample_pdf = \"./resources/JerarquiaDocs.pdf\"\n",
    "# most resource-intensive part\n",
    "converted_result = converter.convert(sample_pdf) # interpreta el contenido y es parseado en un formato estructurado\n",
    "\n",
    "doc = converted_result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoclingDocument(schema_name='DoclingDocument', version='1.0.0', name='JerarquiaDocs', origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None), furniture=GroupItem(self_ref='#/furniture', parent=None, children=[], name='_root_', label=<GroupLabel.UNSPECIFIED: 'unspecified'>), body=GroupItem(self_ref='#/body', parent=None, children=[RefItem(cref='#/texts/0'), RefItem(cref='#/texts/1'), RefItem(cref='#/texts/2'), RefItem(cref='#/texts/3'), RefItem(cref='#/groups/0'), RefItem(cref='#/texts/6'), RefItem(cref='#/texts/7'), RefItem(cref='#/texts/8'), RefItem(cref='#/texts/9'), RefItem(cref='#/texts/10')], name='_root_', label=<GroupLabel.UNSPECIFIED: 'unspecified'>), groups=[GroupItem(self_ref='#/groups/0', parent=RefItem(cref='#/body'), children=[RefItem(cref='#/texts/4'), RefItem(cref='#/texts/5')], name='list', label=<GroupLabel.LIST: 'list'>)], texts=[SectionHeaderItem(self_ref='#/texts/0', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.88172912597656, t=753.07763671875, r=346.6593933105469, b=728.6087646484375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 32))], orig='Non-deterministic nature of llms', text='Non-deterministic nature of llms', level=1), TextItem(self_ref='#/texts/1', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.44147491455078, t=722.78662109375, r=511.21490478515625, b=694.7868041992188, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 110))], orig='Even if you set Temperature as Zero, it is still possible to get different answers because you are using GPUs.', text='Even if you set Temperature as Zero, it is still possible to get different answers because you are using GPUs.'), TextItem(self_ref='#/texts/2', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.27330780029297, t=693.7466430664062, r=502.0074768066406, b=651.3468017578125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 246))], orig='In this case, the model will choose the token with the highest probability. However, when there are multiple words competing probabilities associated super close to each other, due to how GPUs calculate via floating points, it can be a coin toss.', text='In this case, the model will choose the token with the highest probability. However, when there are multiple words competing probabilities associated super close to each other, due to how GPUs calculate via floating points, it can be a coin toss.'), SectionHeaderItem(self_ref='#/texts/3', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=83.71952056884766, t=642.2064819335938, r=375.7834777832031, b=622.7192993164062, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 43))], orig=\"Why LLM's responses vary from time to time?\", text=\"Why LLM's responses vary from time to time?\", level=1), ListItem(self_ref='#/texts/4', parent=RefItem(cref='#/groups/0'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.52487182617188, t=617.1866455078125, r=498.0768127441406, b=589.3067626953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 116))], orig='(1) In addition to the inherent non-deterministic underlying architecture built in the core ethos of modern AI work.', text='(1) In addition to the inherent non-deterministic underlying architecture built in the core ethos of modern AI work.', enumerated=False, marker='-'), ListItem(self_ref='#/texts/5', parent=RefItem(cref='#/groups/0'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.40046691894531, t=580.1966552734375, r=503.4515075683594, b=508.7567443847656, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 383))], orig=\"(2) At the end, models generate next tokens. We see the output as text but inside it is generating a string of numbers and probabilities from the tokens distribution it can pick. For example, GPT-4 has 100,000 tokens it can choose from, so each result is the 100,000 list of model's probabilities to each possible token, selecting the top-k inherent in the configuration metric used.\", text=\"(2) At the end, models generate next tokens. We see the output as text but inside it is generating a string of numbers and probabilities from the tokens distribution it can pick. For example, GPT-4 has 100,000 tokens it can choose from, so each result is the 100,000 list of model's probabilities to each possible token, selecting the top-k inherent in the configuration metric used.\", enumerated=False, marker='-'), TextItem(self_ref='#/texts/6', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=83.91755676269531, t=499.79656982421875, r=500.9917297363281, b=370.3767395019531, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 698))], orig='Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)', text='Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)'), SectionHeaderItem(self_ref='#/texts/7', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.23881530761719, t=361.23358154296875, r=129.5827178955078, b=344.0907287597656, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 6))], orig='Quotes', text='Quotes', level=1), TextItem(self_ref='#/texts/8', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.4170150756836, t=338.8565673828125, r=379.842041015625, b=325.3767395019531, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 62))], orig='\"LLM non-deterministic nature is more of a feature than a bug\"', text='\"LLM non-deterministic nature is more of a feature than a bug\"'), SectionHeaderItem(self_ref='#/texts/9', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.SECTION_HEADER: 'section_header'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.4119873046875, t=316.41656494140625, r=137.9793701171875, b=302.9367370605469, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 10))], orig='Best Quote', text='Best Quote', level=1), TextItem(self_ref='#/texts/10', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.53026580810547, t=299.8565673828125, r=505.6286315917969, b=257.4567565917969, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 248))], orig='\"If you are asking for deterministic results on an LLM you basically do not understand the basics of an LLM. But the good news is you can lower the chances of different answers by setting some parameters, however it still does not be deterministic\"', text='\"If you are asking for deterministic results on an LLM you basically do not understand the basics of an LLM. But the good news is you can lower the chances of different answers by setting some parameters, however it still does not be deterministic\"')], pictures=[], tables=[], key_value_items=[], pages={1: PageItem(size=Size(width=595.3200073242188, height=841.9200439453125), image=None, page_no=1)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable doc es la que se pasará como input a nuestro chunker avanzado. Sin embargo, podemos probar a hacer primeramente un chunking jerárquico de ese doc (hier_chunks no es usado en el resto del documento, el chunker avanzado hará de forma automático este mismo chunking para doc y después hará el refinado por límite de tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_chunks = list(HierarchicalChunker().chunk(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocChunk(text='Even if you set Temperature as Zero, it is still possible to get different answers because you are using GPUs.', meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[TextItem(self_ref='#/texts/1', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.44147491455078, t=722.78662109375, r=511.21490478515625, b=694.7868041992188, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 110))], orig='Even if you set Temperature as Zero, it is still possible to get different answers because you are using GPUs.', text='Even if you set Temperature as Zero, it is still possible to get different answers because you are using GPUs.')], headings=['Non-deterministic nature of llms'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None))),\n",
       " DocChunk(text='In this case, the model will choose the token with the highest probability. However, when there are multiple words competing probabilities associated super close to each other, due to how GPUs calculate via floating points, it can be a coin toss.', meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[TextItem(self_ref='#/texts/2', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.27330780029297, t=693.7466430664062, r=502.0074768066406, b=651.3468017578125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 246))], orig='In this case, the model will choose the token with the highest probability. However, when there are multiple words competing probabilities associated super close to each other, due to how GPUs calculate via floating points, it can be a coin toss.', text='In this case, the model will choose the token with the highest probability. However, when there are multiple words competing probabilities associated super close to each other, due to how GPUs calculate via floating points, it can be a coin toss.')], headings=['Non-deterministic nature of llms'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None))),\n",
       " DocChunk(text=\"(1) In addition to the inherent non-deterministic underlying architecture built in the core ethos of modern AI work.\\n(2) At the end, models generate next tokens. We see the output as text but inside it is generating a string of numbers and probabilities from the tokens distribution it can pick. For example, GPT-4 has 100,000 tokens it can choose from, so each result is the 100,000 list of model's probabilities to each possible token, selecting the top-k inherent in the configuration metric used.\", meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[ListItem(self_ref='#/texts/4', parent=RefItem(cref='#/groups/0'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.52487182617188, t=617.1866455078125, r=498.0768127441406, b=589.3067626953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 116))], orig='(1) In addition to the inherent non-deterministic underlying architecture built in the core ethos of modern AI work.', text='(1) In addition to the inherent non-deterministic underlying architecture built in the core ethos of modern AI work.', enumerated=False, marker='-'), ListItem(self_ref='#/texts/5', parent=RefItem(cref='#/groups/0'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.40046691894531, t=580.1966552734375, r=503.4515075683594, b=508.7567443847656, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 383))], orig=\"(2) At the end, models generate next tokens. We see the output as text but inside it is generating a string of numbers and probabilities from the tokens distribution it can pick. For example, GPT-4 has 100,000 tokens it can choose from, so each result is the 100,000 list of model's probabilities to each possible token, selecting the top-k inherent in the configuration metric used.\", text=\"(2) At the end, models generate next tokens. We see the output as text but inside it is generating a string of numbers and probabilities from the tokens distribution it can pick. For example, GPT-4 has 100,000 tokens it can choose from, so each result is the 100,000 list of model's probabilities to each possible token, selecting the top-k inherent in the configuration metric used.\", enumerated=False, marker='-')], headings=[\"Why LLM's responses vary from time to time?\"], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None))),\n",
       " DocChunk(text='Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)', meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[TextItem(self_ref='#/texts/6', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=83.91755676269531, t=499.79656982421875, r=500.9917297363281, b=370.3767395019531, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 698))], orig='Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)', text='Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)')], headings=[\"Why LLM's responses vary from time to time?\"], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None))),\n",
       " DocChunk(text='\"LLM non-deterministic nature is more of a feature than a bug\"', meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[TextItem(self_ref='#/texts/8', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.4170150756836, t=338.8565673828125, r=379.842041015625, b=325.3767395019531, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 62))], orig='\"LLM non-deterministic nature is more of a feature than a bug\"', text='\"LLM non-deterministic nature is more of a feature than a bug\"')], headings=['Quotes'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None))),\n",
       " DocChunk(text='\"If you are asking for deterministic results on an LLM you basically do not understand the basics of an LLM. But the good news is you can lower the chances of different answers by setting some parameters, however it still does not be deterministic\"', meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[TextItem(self_ref='#/texts/10', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.53026580810547, t=299.8565673828125, r=505.6286315917969, b=257.4567565917969, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 248))], orig='\"If you are asking for deterministic results on an LLM you basically do not understand the basics of an LLM. But the good news is you can lower the chances of different answers by setting some parameters, however it still does not be deterministic\"', text='\"If you are asking for deterministic results on an LLM you basically do not understand the basics of an LLM. But the good news is you can lower the chances of different answers by setting some parameters, however it still does not be deterministic\"')], headings=['Best Quote'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None)))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hier_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Iterable, Iterator\n",
    "\n",
    "from docling_core.transforms.chunker import (\n",
    "    BaseChunk,\n",
    "    BaseChunker,\n",
    "    DocMeta,\n",
    "    HierarchicalChunker,\n",
    ")\n",
    "from docling_core.types.doc import DoclingDocument as DLDocument\n",
    "from pydantic import ConfigDict, PositiveInt\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxTokenLimitingChunker(BaseChunker):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    inner_chunker: BaseChunker = HierarchicalChunker() # setamos el chunker jerárquico como primer chunking\n",
    "    tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "    max_tokens: PositiveInt = 512\n",
    "    delim: str = \"\\n\"\n",
    "\n",
    "    def _serialize_meta_to_include(self, meta: DocMeta) -> str:\n",
    "        meta_parts = []\n",
    "        headings_part = self.delim.join(meta.headings or []) # titulo de la seccion\n",
    "        if headings_part:\n",
    "            meta_parts.append(headings_part)\n",
    "        captions_part = self.delim.join(meta.captions or []) # contenido (sera limitado por los max tokens)\n",
    "        if captions_part:\n",
    "            meta_parts.append(captions_part)\n",
    "        return self.delim.join(meta_parts)\n",
    "\n",
    "    def _split_above_max_tokens(self, chunk_iter: Iterable[BaseChunk]): # segundo tokenizador por limite de tokens\n",
    "        for chunk in chunk_iter:\n",
    "            meta = DocMeta.model_validate(chunk.meta)\n",
    "            meta_text = self._serialize_meta_to_include(meta=meta) # coge los metadatos (titulos y captions)\n",
    "            meta_list = [meta_text] if meta_text else []\n",
    "            full_ser = self.delim.join(meta_list + ([chunk.text] if chunk.text else []))\n",
    "\n",
    "            meta_tokens = self.tokenizer(\n",
    "                meta_text, return_offsets_mapping=True, add_special_tokens=False\n",
    "            )[\"offset_mapping\"]\n",
    "            delim_tokens = (\n",
    "                self.tokenizer(\n",
    "                    self.delim, return_offsets_mapping=True, add_special_tokens=False\n",
    "                )[\"offset_mapping\"]\n",
    "                if meta_text\n",
    "                else []\n",
    "            )\n",
    "            num_tokens_avail_for_text = self.max_tokens - (\n",
    "                len(meta_tokens) + len(delim_tokens)\n",
    "            )\n",
    "\n",
    "            text_tokens = self.tokenizer(\n",
    "                chunk.text, return_offsets_mapping=True, add_special_tokens=False\n",
    "            )[\"offset_mapping\"]\n",
    "            num_text_tokens = len(text_tokens)\n",
    "\n",
    "            if (\n",
    "                num_text_tokens <= num_tokens_avail_for_text\n",
    "            ):  # chunk already within token limit\n",
    "                c = deepcopy(chunk)\n",
    "                c.text = full_ser\n",
    "                yield c\n",
    "            else:  # chunk requires further splitting to meet token limit\n",
    "                fitting_texts = [\n",
    "                    chunk.text[\n",
    "                        text_tokens[base][0] : text_tokens[\n",
    "                            min(base + num_tokens_avail_for_text, num_text_tokens) - 1\n",
    "                        ][1]\n",
    "                    ]\n",
    "                    for base in range(0, num_text_tokens, num_tokens_avail_for_text)\n",
    "                ]\n",
    "                for text in fitting_texts:\n",
    "                    c = deepcopy(chunk)\n",
    "                    c.text = self.delim.join(meta_list + [text])\n",
    "                    yield c\n",
    "\n",
    "    def chunk(self, dl_doc: DLDocument, **kwargs) -> Iterator[BaseChunk]:\n",
    "        chunk_iter = self.inner_chunker.chunk(dl_doc=dl_doc, **kwargs) # procesa el documento y aplica el chunking jerarquico\n",
    "        yield from self._split_above_max_tokens(chunk_iter=chunk_iter) # hace un segundo chunking con limite de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Length: 32\n",
      "RAW Metadata: schema_name='docling_core.transforms.chunker.DocMeta' version='1.0.0' doc_items=[TextItem(self_ref='#/texts/1', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.44147491455078, t=722.78662109375, r=511.21490478515625, b=694.7868041992188, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 110))], orig='Even if you set Temperature as Zero, it is still possible to get different answers because you are using GPUs.', text='Even if you set Temperature as Zero, it is still possible to get different answers because you are using GPUs.')] headings=['Non-deterministic nature of llms'] captions=None origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None)\n",
      "PARSED Metadata Fields:\n",
      "  schema_name: docling_core.transforms.chunker.DocMeta\n",
      "  version: 1.0.0\n",
      "  doc_items:\n",
      "    Item 1:\n",
      "      self_ref: #/texts/1\n",
      "      parent: cref='#/body'\n",
      "      children: []\n",
      "      label: text\n",
      "      prov:\n",
      "        Provenance 1:\n",
      "          page_no: 1\n",
      "          bbox:\n",
      "            l: 84.44147491455078\n",
      "            t: 722.78662109375\n",
      "            r: 511.21490478515625\n",
      "            b: 694.7868041992188\n",
      "            coord_origin: CoordOrigin.BOTTOMLEFT\n",
      "          charspan: (0, 110)\n",
      "      orig: Even if you set Temperature as Zero, it is still possible to get different answers because you are using GPUs.\n",
      "      text: Even if you set Temperature as Zero, it is still possible to get different answers because you are using GPUs.\n",
      "  headings: ['Non-deterministic nature of llms']\n",
      "  captions: None\n",
      "  origin: mimetype='application/pdf' binary_hash=7167666031040985369 filename='JerarquiaDocs.pdf' uri=None\n",
      "Chunk Text: Non-deterministic nature of llms\n",
      "Even if you set Temperature as Zero, it is still possible to get different answers because you are using GPUs.\n",
      "--------------------------------------------------\n",
      "Token Length: 59\n",
      "RAW Metadata: schema_name='docling_core.transforms.chunker.DocMeta' version='1.0.0' doc_items=[TextItem(self_ref='#/texts/2', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.27330780029297, t=693.7466430664062, r=502.0074768066406, b=651.3468017578125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 246))], orig='In this case, the model will choose the token with the highest probability. However, when there are multiple words competing probabilities associated super close to each other, due to how GPUs calculate via floating points, it can be a coin toss.', text='In this case, the model will choose the token with the highest probability. However, when there are multiple words competing probabilities associated super close to each other, due to how GPUs calculate via floating points, it can be a coin toss.')] headings=['Non-deterministic nature of llms'] captions=None origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None)\n",
      "PARSED Metadata Fields:\n",
      "  schema_name: docling_core.transforms.chunker.DocMeta\n",
      "  version: 1.0.0\n",
      "  doc_items:\n",
      "    Item 1:\n",
      "      self_ref: #/texts/2\n",
      "      parent: cref='#/body'\n",
      "      children: []\n",
      "      label: text\n",
      "      prov:\n",
      "        Provenance 1:\n",
      "          page_no: 1\n",
      "          bbox:\n",
      "            l: 84.27330780029297\n",
      "            t: 693.7466430664062\n",
      "            r: 502.0074768066406\n",
      "            b: 651.3468017578125\n",
      "            coord_origin: CoordOrigin.BOTTOMLEFT\n",
      "          charspan: (0, 246)\n",
      "      orig: In this case, the model will choose the token with the highest probability. However, when there are multiple words competing probabilities associated super close to each other, due to how GPUs calculate via floating points, it can be a coin toss.\n",
      "      text: In this case, the model will choose the token with the highest probability. However, when there are multiple words competing probabilities associated super close to each other, due to how GPUs calculate via floating points, it can be a coin toss.\n",
      "  headings: ['Non-deterministic nature of llms']\n",
      "  captions: None\n",
      "  origin: mimetype='application/pdf' binary_hash=7167666031040985369 filename='JerarquiaDocs.pdf' uri=None\n",
      "Chunk Text: Non-deterministic nature of llms\n",
      "In this case, the model will choose the token with the highest probability. However, when there are multiple words competing probabilities associated super close to each other, due to how GPUs calculate via floating points, it can be a coin toss.\n",
      "--------------------------------------------------\n",
      "Token Length: 130\n",
      "RAW Metadata: schema_name='docling_core.transforms.chunker.DocMeta' version='1.0.0' doc_items=[ListItem(self_ref='#/texts/4', parent=RefItem(cref='#/groups/0'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.52487182617188, t=617.1866455078125, r=498.0768127441406, b=589.3067626953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 116))], orig='(1) In addition to the inherent non-deterministic underlying architecture built in the core ethos of modern AI work.', text='(1) In addition to the inherent non-deterministic underlying architecture built in the core ethos of modern AI work.', enumerated=False, marker='-'), ListItem(self_ref='#/texts/5', parent=RefItem(cref='#/groups/0'), children=[], label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.40046691894531, t=580.1966552734375, r=503.4515075683594, b=508.7567443847656, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 383))], orig=\"(2) At the end, models generate next tokens. We see the output as text but inside it is generating a string of numbers and probabilities from the tokens distribution it can pick. For example, GPT-4 has 100,000 tokens it can choose from, so each result is the 100,000 list of model's probabilities to each possible token, selecting the top-k inherent in the configuration metric used.\", text=\"(2) At the end, models generate next tokens. We see the output as text but inside it is generating a string of numbers and probabilities from the tokens distribution it can pick. For example, GPT-4 has 100,000 tokens it can choose from, so each result is the 100,000 list of model's probabilities to each possible token, selecting the top-k inherent in the configuration metric used.\", enumerated=False, marker='-')] headings=[\"Why LLM's responses vary from time to time?\"] captions=None origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None)\n",
      "PARSED Metadata Fields:\n",
      "  schema_name: docling_core.transforms.chunker.DocMeta\n",
      "  version: 1.0.0\n",
      "  doc_items:\n",
      "    Item 1:\n",
      "      self_ref: #/texts/4\n",
      "      parent: cref='#/groups/0'\n",
      "      children: []\n",
      "      label: list_item\n",
      "      prov:\n",
      "        Provenance 1:\n",
      "          page_no: 1\n",
      "          bbox:\n",
      "            l: 84.52487182617188\n",
      "            t: 617.1866455078125\n",
      "            r: 498.0768127441406\n",
      "            b: 589.3067626953125\n",
      "            coord_origin: CoordOrigin.BOTTOMLEFT\n",
      "          charspan: (0, 116)\n",
      "      orig: (1) In addition to the inherent non-deterministic underlying architecture built in the core ethos of modern AI work.\n",
      "      text: (1) In addition to the inherent non-deterministic underlying architecture built in the core ethos of modern AI work.\n",
      "      enumerated: False\n",
      "      marker: -\n",
      "    Item 2:\n",
      "      self_ref: #/texts/5\n",
      "      parent: cref='#/groups/0'\n",
      "      children: []\n",
      "      label: list_item\n",
      "      prov:\n",
      "        Provenance 1:\n",
      "          page_no: 1\n",
      "          bbox:\n",
      "            l: 84.40046691894531\n",
      "            t: 580.1966552734375\n",
      "            r: 503.4515075683594\n",
      "            b: 508.7567443847656\n",
      "            coord_origin: CoordOrigin.BOTTOMLEFT\n",
      "          charspan: (0, 383)\n",
      "      orig: (2) At the end, models generate next tokens. We see the output as text but inside it is generating a string of numbers and probabilities from the tokens distribution it can pick. For example, GPT-4 has 100,000 tokens it can choose from, so each result is the 100,000 list of model's probabilities to each possible token, selecting the top-k inherent in the configuration metric used.\n",
      "      text: (2) At the end, models generate next tokens. We see the output as text but inside it is generating a string of numbers and probabilities from the tokens distribution it can pick. For example, GPT-4 has 100,000 tokens it can choose from, so each result is the 100,000 list of model's probabilities to each possible token, selecting the top-k inherent in the configuration metric used.\n",
      "      enumerated: False\n",
      "      marker: -\n",
      "  headings: [\"Why LLM's responses vary from time to time?\"]\n",
      "  captions: None\n",
      "  origin: mimetype='application/pdf' binary_hash=7167666031040985369 filename='JerarquiaDocs.pdf' uri=None\n",
      "Chunk Text: Why LLM's responses vary from time to time?\n",
      "(1) In addition to the inherent non-deterministic underlying architecture built in the core ethos of modern AI work.\n",
      "(2) At the end, models generate next tokens. We see the output as text but inside it is generating a string of numbers and probabilities from the tokens distribution it can pick. For example, GPT-4 has 100,000 tokens it can choose from, so each result is the 100,000 list of model's probabilities to each possible token, selecting the top-k inherent in the configuration metric used.\n",
      "--------------------------------------------------\n",
      "Token Length: 150\n",
      "RAW Metadata: schema_name='docling_core.transforms.chunker.DocMeta' version='1.0.0' doc_items=[TextItem(self_ref='#/texts/6', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=83.91755676269531, t=499.79656982421875, r=500.9917297363281, b=370.3767395019531, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 698))], orig='Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)', text='Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)')] headings=[\"Why LLM's responses vary from time to time?\"] captions=None origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None)\n",
      "PARSED Metadata Fields:\n",
      "  schema_name: docling_core.transforms.chunker.DocMeta\n",
      "  version: 1.0.0\n",
      "  doc_items:\n",
      "    Item 1:\n",
      "      self_ref: #/texts/6\n",
      "      parent: cref='#/body'\n",
      "      children: []\n",
      "      label: text\n",
      "      prov:\n",
      "        Provenance 1:\n",
      "          page_no: 1\n",
      "          bbox:\n",
      "            l: 83.91755676269531\n",
      "            t: 499.79656982421875\n",
      "            r: 500.9917297363281\n",
      "            b: 370.3767395019531\n",
      "            coord_origin: CoordOrigin.BOTTOMLEFT\n",
      "          charspan: (0, 698)\n",
      "      orig: Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)\n",
      "      text: Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)\n",
      "  headings: [\"Why LLM's responses vary from time to time?\"]\n",
      "  captions: None\n",
      "  origin: mimetype='application/pdf' binary_hash=7167666031040985369 filename='JerarquiaDocs.pdf' uri=None\n",
      "Chunk Text: Why LLM's responses vary from time to time?\n",
      "Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase\n",
      "--------------------------------------------------\n",
      "Token Length: 26\n",
      "RAW Metadata: schema_name='docling_core.transforms.chunker.DocMeta' version='1.0.0' doc_items=[TextItem(self_ref='#/texts/6', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=83.91755676269531, t=499.79656982421875, r=500.9917297363281, b=370.3767395019531, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 698))], orig='Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)', text='Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)')] headings=[\"Why LLM's responses vary from time to time?\"] captions=None origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None)\n",
      "PARSED Metadata Fields:\n",
      "  schema_name: docling_core.transforms.chunker.DocMeta\n",
      "  version: 1.0.0\n",
      "  doc_items:\n",
      "    Item 1:\n",
      "      self_ref: #/texts/6\n",
      "      parent: cref='#/body'\n",
      "      children: []\n",
      "      label: text\n",
      "      prov:\n",
      "        Provenance 1:\n",
      "          page_no: 1\n",
      "          bbox:\n",
      "            l: 83.91755676269531\n",
      "            t: 499.79656982421875\n",
      "            r: 500.9917297363281\n",
      "            b: 370.3767395019531\n",
      "            coord_origin: CoordOrigin.BOTTOMLEFT\n",
      "          charspan: (0, 698)\n",
      "      orig: Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)\n",
      "      text: Then, these probabilities are passed to a SoftMax ( https://www.baeldung.com/wp content/uploads/sites/4/2023/05/softmax_animation.gif) where Temperature is applied, which smooths out the probability distribution, decreasing the likelihood of the most probable token choices and increasing the likelihood of the less likely tokens. After SoftMax and Temperature is applied, one token is picked in accordance with the metrics and that is how you get the token the user sees. (Note: So, with too low temperature the results are more predictable in responses every time but setting it too high you increase the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)\n",
      "  headings: [\"Why LLM's responses vary from time to time?\"]\n",
      "  captions: None\n",
      "  origin: mimetype='application/pdf' binary_hash=7167666031040985369 filename='JerarquiaDocs.pdf' uri=None\n",
      "Chunk Text: Why LLM's responses vary from time to time?\n",
      "the possibilities of receiving nonsense sentences or causing coherent but incorrect responses.)\n",
      "--------------------------------------------------\n",
      "Token Length: 19\n",
      "RAW Metadata: schema_name='docling_core.transforms.chunker.DocMeta' version='1.0.0' doc_items=[TextItem(self_ref='#/texts/8', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.4170150756836, t=338.8565673828125, r=379.842041015625, b=325.3767395019531, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 62))], orig='\"LLM non-deterministic nature is more of a feature than a bug\"', text='\"LLM non-deterministic nature is more of a feature than a bug\"')] headings=['Quotes'] captions=None origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None)\n",
      "PARSED Metadata Fields:\n",
      "  schema_name: docling_core.transforms.chunker.DocMeta\n",
      "  version: 1.0.0\n",
      "  doc_items:\n",
      "    Item 1:\n",
      "      self_ref: #/texts/8\n",
      "      parent: cref='#/body'\n",
      "      children: []\n",
      "      label: text\n",
      "      prov:\n",
      "        Provenance 1:\n",
      "          page_no: 1\n",
      "          bbox:\n",
      "            l: 84.4170150756836\n",
      "            t: 338.8565673828125\n",
      "            r: 379.842041015625\n",
      "            b: 325.3767395019531\n",
      "            coord_origin: CoordOrigin.BOTTOMLEFT\n",
      "          charspan: (0, 62)\n",
      "      orig: \"LLM non-deterministic nature is more of a feature than a bug\"\n",
      "      text: \"LLM non-deterministic nature is more of a feature than a bug\"\n",
      "  headings: ['Quotes']\n",
      "  captions: None\n",
      "  origin: mimetype='application/pdf' binary_hash=7167666031040985369 filename='JerarquiaDocs.pdf' uri=None\n",
      "Chunk Text: Quotes\n",
      "\"LLM non-deterministic nature is more of a feature than a bug\"\n",
      "--------------------------------------------------\n",
      "Token Length: 56\n",
      "RAW Metadata: schema_name='docling_core.transforms.chunker.DocMeta' version='1.0.0' doc_items=[TextItem(self_ref='#/texts/10', parent=RefItem(cref='#/body'), children=[], label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=84.53026580810547, t=299.8565673828125, r=505.6286315917969, b=257.4567565917969, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 248))], orig='\"If you are asking for deterministic results on an LLM you basically do not understand the basics of an LLM. But the good news is you can lower the chances of different answers by setting some parameters, however it still does not be deterministic\"', text='\"If you are asking for deterministic results on an LLM you basically do not understand the basics of an LLM. But the good news is you can lower the chances of different answers by setting some parameters, however it still does not be deterministic\"')] headings=['Best Quote'] captions=None origin=DocumentOrigin(mimetype='application/pdf', binary_hash=7167666031040985369, filename='JerarquiaDocs.pdf', uri=None)\n",
      "PARSED Metadata Fields:\n",
      "  schema_name: docling_core.transforms.chunker.DocMeta\n",
      "  version: 1.0.0\n",
      "  doc_items:\n",
      "    Item 1:\n",
      "      self_ref: #/texts/10\n",
      "      parent: cref='#/body'\n",
      "      children: []\n",
      "      label: text\n",
      "      prov:\n",
      "        Provenance 1:\n",
      "          page_no: 1\n",
      "          bbox:\n",
      "            l: 84.53026580810547\n",
      "            t: 299.8565673828125\n",
      "            r: 505.6286315917969\n",
      "            b: 257.4567565917969\n",
      "            coord_origin: CoordOrigin.BOTTOMLEFT\n",
      "          charspan: (0, 248)\n",
      "      orig: \"If you are asking for deterministic results on an LLM you basically do not understand the basics of an LLM. But the good news is you can lower the chances of different answers by setting some parameters, however it still does not be deterministic\"\n",
      "      text: \"If you are asking for deterministic results on an LLM you basically do not understand the basics of an LLM. But the good news is you can lower the chances of different answers by setting some parameters, however it still does not be deterministic\"\n",
      "  headings: ['Best Quote']\n",
      "  captions: None\n",
      "  origin: mimetype='application/pdf' binary_hash=7167666031040985369 filename='JerarquiaDocs.pdf' uri=None\n",
      "Chunk Text: Best Quote\n",
      "\"If you are asking for deterministic results on an LLM you basically do not understand the basics of an LLM. But the good news is you can lower the chances of different answers by setting some parameters, however it still does not be deterministic\"\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# inicializar MaxTokenLimitingChunker con los ajustes deseados\n",
    "complete_chunker = MaxTokenLimitingChunker(\n",
    "    inner_chunker=HierarchicalChunker(), # hierarchical chunker\n",
    "    max_tokens=150  # token limit\n",
    ")\n",
    "\n",
    "final_chunks = list(complete_chunker.chunk(doc)) # primero se hace el chunking jerarquico y despues el del limitador de tokens\n",
    "\n",
    "# final chunks ya contiene los chunks jerarquicos + con limite de tokens\n",
    "for chunk in final_chunks:\n",
    "    meta = DocMeta.model_validate(chunk.meta) # cogemos los metadatos\n",
    "    # calcular el tamaño\n",
    "    token_length = len(complete_chunker.tokenizer(chunk.text, return_offsets_mapping=True, add_special_tokens=False)[\"offset_mapping\"])\n",
    "    \n",
    "    # Display de los datos\n",
    "    print(f\"Token Length: {token_length}\")\n",
    "    print(\"RAW Metadata:\", meta)\n",
    "    print(\"PARSED Metadata Fields:\")\n",
    "    for field, value in meta.__dict__.items():\n",
    "        if field == \"doc_items\":\n",
    "            print(f\"  {field}:\")\n",
    "            for idx, item in enumerate(value):\n",
    "                print(f\"    Item {idx+1}:\")\n",
    "                for item_field, item_value in item.__dict__.items():\n",
    "                    if item_field == \"prov\":\n",
    "                        print(f\"      {item_field}:\")\n",
    "                        for prov_idx, prov_item in enumerate(item_value):\n",
    "                            print(f\"        Provenance {prov_idx+1}:\")\n",
    "                            for prov_field, prov_value in prov_item.__dict__.items():\n",
    "                                if prov_field == \"bbox\":\n",
    "                                    print(f\"          {prov_field}:\")\n",
    "                                    for bbox_field, bbox_value in prov_value.__dict__.items():\n",
    "                                        print(f\"            {bbox_field}: {bbox_value}\")\n",
    "                                else:\n",
    "                                    print(f\"          {prov_field}: {prov_value}\")\n",
    "                    else:\n",
    "                        print(f\"      {item_field}: {item_value}\")\n",
    "        else:\n",
    "            print(f\"  {field}: {value}\")\n",
    "    print(\"Chunk Text:\", chunk.text)\n",
    "    print(\"-\" * 50)  # Separador para la lectura de entre los chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
